---
title: "Preregistration FLO paper"
author:
- Eva Viviani
- Michael Ramscar
- Elizabeth Wonnacott
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    df_print: paged
  word_document: default
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

# Study information
In this preregistration we describe a detailed plan to replicate @ramscar2010effects paper about the effect of the order of presentation of feature-label pair association on symbolic learning. 
We plan to translate the experiment in an online setting and to introduce a contingency judgment task that would link the theoretical predictions about error-driven learning, i.e., cue-competition, to the participants' ratings behaviour of the feature-label association plausibility. 

We expect that this commitment will provide (1) a robust paradigm that can be used as starting point for future studies online, (2) a sensitive task capturing estimates of cue-competition across participants, providing a direct comparison to the predictions based on the RW model (@rescorla1972theory). 
We base our predictions on data of an unpublished study that replicates @ramscar2010effects effects and on our own pilot data.

<!--

We consider two possible ways in which symbols might be learned: by learning to predict a label from the features of objects (cue-outcome), and by learning to predict features from a label (outcome-cue). We expect better learning with a discriminative (cue–outcome) order compared to a non-discriminative (outcome–cue) order. To test this, we vary the frequency distribution of predictive and uninformative cues associated to three labels. Each label has two high salient, but not predictive cues -- body shapes, and two low salient but predictive cues -- shapes attached to the body. One combination of high and low salience cues are associated to a label in the 75% of the cases (high frequency), while another in the 25% of the cases (low frequency). High salient cues are shared across labels and therefore are not predictive of the label category. For example, a cues used in the high frequency condition of label 1, could be used in the low frequency condition of label 2. This was not the case for the low-saliency cues which are uniquely associated to its label.


 These predictions are based on @ramscar2010effects data concerning differences in learning between the two trainings, in turn supported by computational simulations revealing that feature-label ordering induces cue-competition, while label-feature order does not promote this error-driven type of learning, leading to think that competing outcomes are all highly probable. 
 
 -->


# Research Questions

Overarching question: Does Feature-Label-Ordering training affect discrimination learning?

In particular, when participants are confronted with novel exemplars, 

- A) Does frequency of occurrence during learning affect generalization of predictive cues? 

- B) Does feature-label and label-feature order affect generalization of predictive cues?

- C) Is there an advantage in terms of generalization between feature-label and label-feature when the frequency of presentation of the predictive cues is low?
  
# Hypotheses
We translate our predictions in terms of average accuracy estimated in two tests previously implemented by @ramscar2010effects, label-picture and picture-label tasks. These tasks require participants to select the right label-feature association among four alternatives constituted by novel exemplars build with the same cues present in the learning.

We hypothesize: 

A) **A main effect of frequency:** the association between predictive cue and its correct label in the low frequency condition will be extremely challenging compared to the high frequency condition regardless of the learning. Therefore we expect higher accuracy in the high vs low frequency condition. This effect has been reported by the study that replicates @ramscar2010effects, with the results for both generalization tasks: [$\beta$ = 1.7, SE = 0.24, z value = 6.85, *p*  $<$ 0.0001].

B) **A main effect of learning:** the association between predictive cue and its correct outcome will be effectively learned only if the training enhances discriminative learning, that is, only in the feature-label training. Therefore we predict higher accuracy in the feature-label learning, compared to label feature learning. From the study that replicates @ramscar2010effects, with the results for both generalization tasks: [$\beta$ = 0.66, SE = 0.14, z value = 4.87, *p*  $<$ 0.0001].

C) **An interaction between frequency and learning:** From the study that replicates @ramscar2010effects, with the results for both generalization tasks: [$\beta$ = -1.02, SE = 0.26, z value = -3.92, *p*  = 0.0001]. Specifically, we predict higher accuracy in the low frequency condition for the feature-label learning group compared to label-feature, [$\beta$ = 1.17, SE = 0.15, z value = 7.47, *p*  < 0.0001], but no difference in the high frequency condition, [$\beta$ = 0.15, SE = 0.21, z value = 0.7, *p*  = 0.48]. Note that this latter prediction stems from the replication data we have, although originally @ramscar2010effects did found a difference between learning in this condition as well.

D) We also predict from @ramscar2010effects that average accuracy in the condition where the association between fribble and label is 100% will be around 1 for the majority of the participants (around 80%), regardless of the learning. This condition will used mainly as sanity check over the whole experimental procedure, and in the analysis later on as exclusion criteria (see analysis plan).



We will also attempt to measure cue competition in a contingency judgment task, for potential sources of comparisons inherent to the RW computational model (@rescorla1972theory). In this task participants are required to express their confidence on a scale from -100 to +100 regarding the label-feature association. The possible combinations between label and features are of two types:

- match: in this scenario the label-fribble pair presented is 100% correct. The fribble's high and low frequency cues match perfectly the label category.

- mismatch-type1: in this scenario the label-fribble pair is partially incorrect, i.e., only the high saliency feature is correct. The fribble posses a high salience feature (body shape) that was previously associated to the label during learning, but the low salience features instead were associated to another.

**Match scenario**

For this scenario, the computational model suggests that we should expect only positive ratings because the cue-outcome association is always positively reinforced. From our pilot data, we hypothesize an average difference size of +11 in each comparison.

Specifically, we expect an interaction between frequency and learning: 

(1) average ratings in the high frequency condition will be positive and higher for the label-feature group compared to the feature-label. 

(2) average ratings in the low frequency condition will be positive and higher for the feature-label compared to label-feature group.

**Mismatch-type1** 

The computational model predicts that the high saliency feature will be downweighted because not 100% predictive of the label. Furthermore, the downweighting of unreliable (non-discriminative) cues, is a peculiarity of the feature-label group only, and this will be evident when the low saliency (but highly predictive) cues belong to the low frequency condition of another label. Following this reasoning, we predict an interaction between frequency and learning. Specifically, we expect:

(1) Average ratings in the low frequency condition will be positive for the label-feature group (+11), while negative for the feature-label (-11). 

(2) Average ratings in the high frequency condition will be negative for both learnings, but larger for feature-label, hypothesized $\Delta$ between conditions of -11.



# Sampling plan

## Existing data
Registration prior to creation of data.

## Data collection procedures
- Participants will be tested via Gorilla (https://gorilla.sc/), and recruited through Prolific (https://gorilla.co/). Participants will complete independently the experiment online.

- All adults with consent to participate will be tested, however some exclusion criteria will apply for the analysis. In line with @ramscar2010effects, those that will score less or equal to 80% on the control condition will be removed from the analysis. 

- Payment of £2 per session will be made to each adult participant (7£ per hour). Adult participants will be offered the full amount at the end of the session only.

## Sample size
- This study aim to test max 90 adults English native speakers. The sample size is informed by previous similar studies conducted online: @nixon2020mice study (Experiment 2) with adults (93 participants), and @vujovic2019 (84 participants). 

## Stopping rule
- Using Bayesian statistics, we are going to collect data until the Bayes factor is larger than 3. We will look at the data after 25 participants in each Learning Group (feature-label/label-feature). If the Bayes factor is larger than 3, we will stop testing.

# Variables
## Manipulated variables

Learning:

-	Label-to-Feature (LF) learning, i.e., learning to predict the objects from the labels.

-	Feature-to-Label (FL) learning, i.e., learning to predict the labels from the objects.

Frequency:

-	High frequency exemplars (highFreq): 75% of exemplars are made of a high and low salient features distinctive of the subcategory.

-	Low frequency exemplars (lowFreq): 25% of exemplars are made of a high salient feature drawn from another label subcategory but a low salient feature distinctive of the subcategory.

## Measured variables

Generalization tasks.

Accuracy on a speeded four-alternative forced choice test (4AFC) where participants:

- Match an unseen exemplar to the four category labels (56 trials, among which 8 controls)

- Match a label with four unseen exemplars (56 trials, among which 8 controls)

Contingency judgment task.

Judgement ratings where participants estimate the strength of the association between a label and an unseen exemplar. Trials will be 100% match between label and the exemplars (match, 24 trials), partial match between label and exemplars (mismatch-type1, 24 trials), 8 control trials. Total: 56.

Half of the participants will match an unseen exemplars to the three category, and the other half will match a label with three novel exemplars. All participants will express their ratings as final task.

# Design Plan
## Study type

Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials.

## Blinding

Participants will not know the learning group to which they have been assigned, neither the frequency manipulation.

## Study design
In the GLMMs (generalization tasks) and LMM (contingency judgment task), *frequency* and *learning* will be specified as fixed effects, and *(frequency|subjects)* as random factor. For the contingency judgment task, we are going to run two separate LM models for the type of trial (match and mismatch-type1). For the GLMMs we are going to specify a binomial distribution given the binary nature of the dependent variable (accuracy). For the Bayesian analyses we will be computing Bayes factors following the approach in @dienes2008understanding. We need a mean and SE to summarize the data and these will come from the estimates and standard errors of the relevant coefficients in the LMM and GLMMs. We inform H1 using data from a replication study of the @ramscar2010effects paper, and our own pilot data for the contingency judgment task. We will specify a half normal distribution in the Bayes Factor computation.

## Randomization
Participants will be randomly allocated to one of the two learning conditions (feature-label or label-feature). The learning sequence presented is randomized for each participant and presented in two blocks. No other cover task is performed. 

# Analysis Plan
## Statistical models
For the Bayesian analyses we will be computing Bayes factors using the method advocated by @dienes2008understanding. We will also alongside provide frequentist statistics. We work in log odds space to meet assumptions of normality and will model the H1 by using estimate of the mean for theory as the SD of a half normal (for one tailed) distribution. 

Logistics Mixed Effects Models:

LMEs will be used to compute frequentist statistics in the picture-label and label-picture tasks. The choice of method is due to the nature of the dependent variable of correct (1) and incorrect (0) response. Frequency by participant effects are included in the mixed models. This is because frequency is expected to greatly vary across participants. Picture-label and label-picture task will be considered together in the analysis if no significant differences emerge across the two tasks.

Linear Mixed Effect models:

LMMs will be used to compute frequentist statistics in the contingency judgment task. The choice of method is due to the continuous nature of the dependent variable. Frequency by participant effects are included in the mixed models. This is because frequency is a within participants variable. Match and mismatch-type1 trials will be considered in separate models.


We will then run pairwise comparisons with learning and frequency as predictors. 

## Follow-up analyses

None.

## Inference criteria
We will base our inferences only on the Bayes analysis. We will continue to work in log odds space (as for Frequentist) to meet assumptions of normality, using estimates and standard errors which come from the logistic mixed effects models. Following @dienes2008understanding, we test one sided predictions, using estimates as the SD of a half normal distribution that will be derived from LMEs for intercept (effect of learning in each Condition) and interaction (main effect of Experiment and Age Group). Note that this means that the maximum we might expect is twice our estimate. In some cases we compute SD using knowledge of constraints on the likely maximum value. (Note- this approach differs from the approach of setting a uniform maximum, in that it favors smaller values, which seems more appropriate).

-	The estimates for each key analyses are already given in the Hypothesis section, for each specific hypothesis listed.

-	We will say we have substantial evidence for H1 if BF is larger than 3 and for H0 if BF is less than .33. 

## Data exclusion
Participants that will score less or equal to 80% in the control condition will be excluded from the analysis.

## Missing data
If more than 10% of the responses at either test tasks from a participant are missing, that participant is excluded from the analyses.

## Exploratory analysis
We will run a correlation between the generalization tasks and the contingency judgment task.


# References
