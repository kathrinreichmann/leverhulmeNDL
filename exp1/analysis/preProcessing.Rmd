---
title: "FLO replication - Preprocessing + analysis + results summary"
author: "Eva"
date: "4/3/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_float: yes
---



```{r loading libraries, echo=FALSE, message=FALSE}
library(tidyverse);
library(httr);
library(RCurl);
library(ggplot2);
library(ggpubr);
library(RColorBrewer)
library(gridExtra);
library(ggExtra);
library(lmerTest);
library(emmeans);
```

# clean WS, set WD 

```{r}
rm(list = ls());
```

Set your local working directory. This should be (and is assumed to be in the rest of the code) the highest point in your local folder:
```{r}
localGitDir <- 'C:/Users/eva_v/Documents/GitHub/leverhulmeNDL'
#setwd(localGitDir);
```
```{r load stimuli set}
fribbleSet <- read.csv(paste(localGitDir, "/exp1/stimuli/stimuli.csv", sep = ""), 
                       header = T,
                       colClasses=c("cueID"="factor",
                        "bodyShape"="factor",
                        "label"="factor",
                        "fribbleID"="factor"
                        ));
```

# Load functions from the lab repo

```{r}
urlFolder <- 'https://api.github.com/repos/n400peanuts/languagelearninglab/git/trees/master?recursive=1'
urlRaw <- 'https://raw.githubusercontent.com/n400peanuts/languagelearninglab/master/tools/'

loadFunctionsGithub <-function(urlFolder, urlRaw){
  if (!require(httr)) {
    stop("httr not installed")
  } 
  else if (!require(RCurl)){
    stop("RCurl not installed") 
  }
  else {
    print('----loading. Please wait----')
  };
  httr::GET(urlFolder)-> req
  stop_for_status(req)
  filelist <- unlist(lapply(content(req)$tree, "[", "path"), use.names = F)
  urlFunctions <- grep("docs/tools/", filelist, value = TRUE, fixed = TRUE)
  gsub("docs/tools/", "", urlFunctions) -> functions
  for (i in 1:length(functions)){
    RCurl::getURL(paste0(urlRaw, functions[i]), ssl.verifypeer = FALSE)-> temp
    eval(parse(text = temp), envir = .GlobalEnv)
  };
}

loadFunctionsGithub(urlFolder = urlFolder, urlRaw = urlRaw);
rm(urlFolder, urlRaw)
```


# Check stimuli set
It's important to check that every fribble is unique in the way its features are assembled within each category.
Feature position and identity are coded into cueID.

I'm going to check whether the combination of cues used to build the fribble is unique by filtering for n > 1:

```{r check for duplicates}

fribbleSet %>%
  group_by(category, cueID) %>%
  count() %>%
  filter(n > 1);
```

Great, each Fribble is unique!

# Load data

List the files present in the folder, and load them.

```{r list files}
df <- list.files(paste(localGitDir, "/exp1/data/", sep = "")); 

```

We have `r length(df)` files.

```{r load files}
for (i in 1:length(df)){
  gsub(".csv$", "", df[i]) -> id
  assign(id, data.frame())
  read.csv(paste(localGitDir, "/exp1/data/", df[i], sep = ""),
           na.strings=c("","NA"),
           colClasses=c("presentedLabel"="factor",
                        "presentedImage"="factor",
                        "learningType"="factor",
                        "Trial.Type"="factor",
                        "Test.Part"="factor",
                        "Key.Press"="factor"
                        ))-> temp
  assign(paste0(id), temp)
};

rm(temp, df, i, id);
```

The dataset name is decided autonomously by Gorilla. Importantly, Gorilla produces a different file per condition, and codes the conditions by the last 4 letters.

- 2yjh is the FL learning

- q8hp is the LF learning

I'm going to rename them for clarity.

```{r rename dataset}
dataFL<-`data_exp_15519-v13_task-2yjh`
dataFL$Experiment.Version <- c(14)
dataFL2<-`data_exp_15519-v14_task-2yjh`
dataFL3<-`data_exp_15519-v15_task-2yjh`

rm(`data_exp_15519-v13_task-2yjh`)
rm(`data_exp_15519-v14_task-2yjh`)
rm(`data_exp_15519-v15_task-2yjh`)

dataLF <- `data_exp_15519-v13_task-q8hp`
dataLF$Experiment.Version <- c(14)
dataLF2 <- `data_exp_15519-v14_task-q8hp`
dataLF3 <- `data_exp_15519-v15_task-q8hp`

rm(`data_exp_15519-v13_task-q8hp`)
rm(`data_exp_15519-v14_task-q8hp`)
rm(`data_exp_15519-v15_task-q8hp`)

```

```{r merge datasets}
rbind(dataFL, dataFL2, dataFL3)-> dataFL
rbind(dataLF, dataLF2, dataLF3)-> dataLF

rm(dataFL2, dataFL3, dataLF2, dataLF3)
```


Gorilla's output is extremely messy. Each row is a screen event. However, we want only the events related to 1. the presentations of the fribbles and the labels 2. participants' response and 3. what type of tasks.

I have coded these info in some columns and rows that I'm going to select:


```{r column selection}
raw_dataFL<- dataFL[c('Participant.Private.ID', 'learningType', 'Test.Part' , 
         'presentedImage', 'presentedLabel', 'Reaction.Time', "Key.Press",
          'Trial.Type', 'Trial.Index', 'Correct', 'Experiment.Version')]

raw_dataLF<- dataLF[c('Participant.Private.ID', 'learningType', 'Test.Part' , 
         'presentedImage', 'presentedLabel', 'Reaction.Time', "Key.Press",
          'Trial.Type', 'Trial.Index', 'Correct', 'Experiment.Version')]


```

Select rows:

```{r rows selection}
rowsIwantTokeep <- c("learningBlock1", "learningBlock2", "learningBlock3",
                        "learningBlock4", "generalizationPL", "generalizationLP",
                        "randomDot", "contingencyJudgement")

raw_dataFL <- raw_dataFL %>% 
  filter(Test.Part %in% rowsIwantTokeep ) %>%
  rename(subjID = Participant.Private.ID, 
         learning = learningType,
         task = Test.Part, 
         fribbleID = presentedImage,
         label = presentedLabel, 
         rt = Reaction.Time, 
         resp = Key.Press, 
         trialType = Trial.Type,
         trialIndex = Trial.Index,
         acc = Correct)

raw_dataLF <- raw_dataLF %>% 
  filter(Test.Part %in% rowsIwantTokeep ) %>%
  rename(subjID = Participant.Private.ID, 
         learning = learningType,
         task = Test.Part, 
         fribbleID = presentedImage,
         label = presentedLabel, 
         rt = Reaction.Time, 
         resp = Key.Press, 
         trialType = Trial.Type,
         trialIndex = Trial.Index,
         acc = Correct)

rm(rowsIwantTokeep, dataFL, dataLF);
```

I'm going to merge both datasets, FL and LF, because we have anyway a column "learning" that can tell us which one is which.

```{r}
rbind(raw_dataFL, raw_dataLF)-> raw_data; 
rm(raw_dataFL, raw_dataLF);

```

# Check learning

Let's filter and check learning trials:

```{r learning filter}
learningBlocks <- c("learningBlock1", "learningBlock2", "learningBlock3", "learningBlock4");

learning <- raw_data %>% 
  filter(task %in% learningBlocks) 

learning <- droplevels(learning);
rm(learningBlocks)
```

## How many trials per participant? 
```{r check number of trials per ppts}
learning %>%                             
  group_by(subjID, learning) %>%    
  count() 
```

Great, 120 trials per participant, per learning.

Let's check whether the blocks' length varied across participants:

```{r check learning blocks length}
learning %>%                             
  group_by(subjID, task) %>%
  count()
```

Great! Each participant had a different amount of trials distributed across blocks. That's important because our random dot task was presented at the end of each block, and we wanted its presentation to be unpredictable.
Anyway, the sum of all the learning trials was always 120.


## Did we assign our learning randomly every couple of people?

```{r check learning randomization and grouping}
table(learning$subjID, learning$learning)
```
Kind of.
After chicking with Gorilla's suppoert: apparently, if a participant access Gorilla, but it's not allowed to start the experiment (e.g., the browser is not suitable), or leaves the session, this counts anyway for the randomisation.

The rows related to the presentation of fribbles and labels, inherit Gorilla's http address of where they are stored. Nothing I can do to change this in Gorilla, but we can clean the rows by those info like this:

```{r rows cleaning}
as.factor(gsub("/task/70033/56/asset/|/task/70033/57/asset/|/task/70033/58/asset/", "", learning$fribbleID))-> learning$fribbleID
as.factor(gsub(".jpg$", "", learning$fribbleID))-> learning$fribbleID

as.factor(gsub("/task/70033/56/asset/|/task/70033/57/asset/|/task/70033/58/asset/", "", learning$label))-> learning$label
as.factor(gsub(".mp3$", "", learning$label))-> learning$label
learning$resp <- as.factor('NA')
```

This is how the learning dataframe looks like now:

```{r}
head(learning);
```

```{r}
summary(learning);
```

Our fribbles were presented two times during learning. 

## Check if fribbles are presented > 2 times:
```{r}
learning %>%                             
  group_by(subjID, fribbleID) %>%    
  count() %>%
  filter(n >2)

```

None, perfect.

## Check whether there are fribbles presented only once:

```{r}
learning %>%                             
  group_by(subjID, fribbleID) %>%    
  count() %>%
  filter(n < 2)
```

Perfect.

## Check the association between the fribbles and the labels (high and low frequency with the correct labels)

Fribbles ID are coded in this way: 
e.g., 10175-> [1] is the category [01] is the number of the fribble [75] is the frequency.

In the column fribbleID we have the fribble presented, in the column label we have the sound played.

Association between fribbles and labels are fixed:

- category 1, regardless of the frequency, has the label: dep

- category 2, regardless of the frequency, has the label: bim

- category 3, regardless of the frequency, has the label: tob

I'm going to add a column for category, fribble number, and frequency, in order to check whether everything is okay:

We should have only 3 categories, presented twice per participant. Each category is made of 20 exemplars.

```{r check label fribble association during learning}
learning$category <- 0
learning[substr(as.character(learning$fribbleID), 1, 1)==1,]$category <- 1
learning[substr(as.character(learning$fribbleID), 1, 1)==2,]$category <- 2
learning[substr(as.character(learning$fribbleID), 1, 1)==3,]$category <- 3

(nrow(learning[learning$category==1,]) / length(unique(learning$subjID))) / 2
(nrow(learning[learning$category==2,]) / length(unique(learning$subjID))) / 2
(nrow(learning[learning$category==3,]) / length(unique(learning$subjID))) / 2

```
We have 15 high frequency and 5 low frequency exemplars x category:

```{r}
learning$frequency <- 25
learning[substr(as.character(learning$fribbleID), 4, 5)==75,]$frequency <- 75

(nrow(learning[learning$frequency==25,]) / length(unique(learning$subjID))) / 2
(nrow(learning[learning$frequency==75,]) / length(unique(learning$subjID))) / 2

```

Now let's check the fribble-label association:

```{r fribble-label table}
table(learning$category, learning$label, learning$frequency)
```

Okay, each label was associated to its correct fribble (coded here as category).


# Check Testing

I'm going to select the tests and clean the rows from Gorilla's http address:

```{r tests sanity check}
tests <- c("generalizationPL", "generalizationLP", "contingencyJudgement", "randomDot");

testing <- raw_data %>% 
  filter(task %in% tests)  


testing <- droplevels(testing);
rm(tests);

as.factor(gsub("/task/70033/56/asset/|/task/70033/57/asset/|/task/70033/58/asset/", "", testing$fribbleID))-> testing$fribbleID
as.factor(gsub(".jpg$", "", testing$fribbleID))-> testing$fribbleID

as.factor(gsub("/task/70033/56/asset/|/task/70033/57/asset/|/task/70033/58/asset/", "", testing$label))-> testing$label
as.factor(gsub(".mp3$", "", testing$label))-> testing$label


```

## Check test 1: Generalization from picture to labels

We filter the rows for this task, and clean both the resp and fribble columns.

```{r loading task}
generalizationPL <- testing %>%
  filter(task == 'generalizationPL') 
generalizationPL <- droplevels(generalizationPL);

as.factor(gsub("/task/70033/56/asset/|/task/70033/57/asset/|/task/70033/58/asset/", "", generalizationPL$resp))-> generalizationPL$resp
as.factor(gsub(".mp3$", "", generalizationPL$resp))-> generalizationPL$resp
as.factor(gsub(".jpg", "", generalizationPL$resp))-> generalizationPL$resp

gsub('[[:punct:]]|"', "", generalizationPL$label)-> generalizationPL$label 

as.factor(gsub('mp3', "_", generalizationPL$label))-> generalizationPL$label

```

### Check how many trials participants

```{r check num of trials}
generalizationPL %>%                             
  group_by(subjID) %>%  
  count() 
```

Great, 24 trials per participant. 

### Check whether participants saw a unique fribble:

```{r check fribbles num of presentation}
generalizationPL %>%                             
  group_by(subjID, fribbleID) %>%  
  count() %>%
  filter(n > 1)
```

Great! 

Integrate stimuli info.
In the file "fribbleSet" I have listed all the fribbles ID and their category, along with their cueIDs and body shape. I'm going to add those columns by merging the test file with the fribbleSet by fribbleID. The rest of the file is left untouched.

```{r merge with stimuli}
merge(generalizationPL, fribbleSet, by = 'fribbleID')-> generalizationPL;
generalizationPL$label.y <- NULL;

generalizationPL <- rename(generalizationPL, label = label.x);

```


Let's check the responses they made, just to see if they make sense.

For example, we want the resp column to be one of the labels.
```{r check resp column}
generalizationPL %>%                             
  group_by(subjID, resp) %>%  
  count() 
```

Great, some participant missed some trials (coded as NA), but that's okay.

So far, so good.


### Check trial/stimuli per category, per frequency, per subject

We have 24 trials per participant, but within those trials we *should* have 8 trials per category, 4 low frequency and 4 high frequency trials.

```{r}
head(table(generalizationPL$subjID, generalizationPL$category, generalizationPL$frequency))
```


Let's check the second task.

## Check test 2: Generalization from label to pictures

```{r from label to pics}
generalizationLP <- testing %>%
  filter(task == 'generalizationLP') 
generalizationLP <- droplevels(generalizationLP)
```

### How many trials per participant?

```{r}
generalizationLP %>%                             
  group_by(subjID) %>%  
  count() 
```

24 trials, great.



### Check whether participants saw a unique fribble

First let's clean the rows from Gorilla gibberish.

```{r}
as.factor(gsub('[[:punct:]]|"', "", generalizationLP$fribbleID))-> generalizationLP$fribbleID 

as.factor(gsub('jpg', "_", generalizationLP$fribbleID))-> generalizationLP$fribbleID

as.factor(gsub("/task/70033/56/asset/|/task/70033/57/asset/|/task/70033/58/asset/", "", generalizationLP$resp))-> generalizationLP$resp

as.factor(gsub(".jpg", "", generalizationLP$resp))-> generalizationLP$resp

```

Then check for duplicates:
```{r}

substr(as.character(generalizationLP$fribbleID), 1, 5)-> temp
substr(as.character(generalizationLP$fribbleID), 7, 11)-> temp2
substr(as.character(generalizationLP$fribbleID), 13, 17)-> temp3

fribblePresented <- c(temp,temp2,temp3)
unique(generalizationLP$subjID)-> subj

duplicatedFribbles <- NA;
for (i in 1:length(subj)){
  substr(as.character(generalizationLP[generalizationLP$subjID==subj[i],]$fribbleID), 1, 5)-> temp
  substr(as.character(generalizationLP[generalizationLP$subjID==subj[i],]$fribbleID), 7, 11)-> temp2
  substr(as.character(generalizationLP[generalizationLP$subjID==subj[i],]$fribbleID), 13, 17)-> temp3
  fribblePresented <- c(temp,temp2,temp3)
  dup <- fribblePresented[duplicated(fribblePresented)] #extract duplicated elements
  print(subj[i])
  
  if (length(dup)>0){
    print(dup)
  } else {
    print(length(dup))
  }
  
};

rm(subj, temp, temp2, temp3, i, fribblePresented, duplicatedFribbles, dup)
```

Great! participants saw always different fribble.

### Check whether fribbles presented were either high or low frequency within trials

In this task we have three pictures and one label pronounced. This means that the fribbleID column contains 3 images.
I'm going to cycle over the dataset, and break the fribbleID column in three, then I'm going to print the fribble that within the same trial has a different frequency. I'm going to print the fribbles that are presented wrongly, e.g., "low high low" etc. If all fribbles are presented correctly: , e.g., "low low low" and "high high high", then the output is empty. 

```{r check fribble presentation low vs high freq}
unique(generalizationLP$subjID)-> subj;

trials <- NULL;
task <- NULL;

for (i in 1:length(subj)){
  as.integer(substr(as.character(generalizationLP[generalizationLP$subjID==subj[i],]$fribbleID), 4, 5))-> temp #first fribble
  as.integer(substr(as.character(generalizationLP[generalizationLP$subjID==subj[i],]$fribbleID), 10, 11))-> temp2 #second fribble
  as.integer(substr(as.character(generalizationLP[generalizationLP$subjID==subj[i],]$fribbleID), 16, 17))-> temp3 #third fribble
trials <- cbind(temp, temp2, temp3, as.integer(subj[i])) # store it in columns along with subj info
task <- rbind(task, trials) #store all subjs
};

for (i in 1:nrow(task)){ #check by rows whether there is a unique number, print the row if wrong
  if ((task[i,1] == task[i,2] & task[i,3])== FALSE) {
    print('wrong frequency fribble:')
    print(task[i,1], task[i,2], task[i,3])
  } 
};

frequency <- ifelse(substr(as.character(task[,1]), 1, 1)==2, 'low', 'high')
cbind(task, frequency)->task
as.data.frame(task)-> task
rm(trials, i, subj, temp, temp2, temp3);
```

Great, fribbles presented were either low or high frequency.
Check whether participants saw 4 trials with low and 4 trials with high frequency:

### Check trial distribution per frequency:

```{r}
head(table(task$V4, task$frequency))
```


I'm going to merge the stimuli set now.

When we do it, this time we need to merge by resp and not by fribbleID, because our fribble selected is coded in this column:

```{r merge stimuli LP}
fribbleSet$resp <- fribbleSet$fribbleID # column's name needs to be the same in order to merge
merge(generalizationLP, fribbleSet, by = 'resp', all.x = T)-> generalizationLP;
fribbleSet$resp <- NULL;
generalizationLP$fribbleID.y <- NULL;
generalizationLP$label.y <- NULL;
generalizationLP <- rename(generalizationLP, label = label.x);
generalizationLP <- rename(generalizationLP, fribbleID = fribbleID.x);

```

### Check responses distribution by category:

```{r}
generalizationLP %>%                             
  group_by(subjID, category) %>%  
  count()
```

Cool.

### Check responses distribution by frequency:

```{r}
generalizationLP %>%                             
  group_by(subjID, label, frequency) %>%  
  count()
```


## Check test 3: Contingency Judgement task

```{r}
contingencyJudgement <- testing %>%
  filter(task == 'contingencyJudgement') 
contingencyJudgement <- droplevels(contingencyJudgement)
```

### How many trials per participant?

```{r}
contingencyJudgement %>%                             
  group_by(subjID) %>%  
  count() 
```

Very good. 

### Did participants see a fribble more than once?

```{r}
droplevels(contingencyJudgement) %>%                             
  group_by(subjID, fribbleID) %>%  
  count() %>%
  filter( n > 1)

```
No! that's great.

### Are labels repeated equally?

```{r}
table(contingencyJudgement$subjID, contingencyJudgement$label)
```

good

```{r merge stimuli contingency task} 
merge(contingencyJudgement, fribbleSet, by = 'fribbleID')-> contingencyJudgement
contingencyJudgement$label.y <- NULL;
contingencyJudgement <- rename(contingencyJudgement, label = label.x)
```

### Check category presentation:

```{r}
contingencyJudgement %>%                             
  group_by(subjID, category) %>%  
  count()
```

```{r}
table(contingencyJudgement$category, contingencyJudgement$label)

```


## Check test 4: Random dot task

Let's check our random dot task. This was inserted randomly during trials 4 times. 5 trials each time, plus 4 practice trials.


```{r random dot filter}
randomDot <- testing %>%
  filter(task == 'randomDot') 

```

### How many trials per participant?

```{r}
randomDot %>%                             
  group_by(subjID) %>%  
  count() 

```

we have 5 trials repeated during learning four times (20) plus 4 practice trials.

### How was accuracy distributed across participants?

First, let's consider that when we have a timeout, the output is -1

```{r}
randomDot %>%                             
  group_by(subjID, resp) %>% 
  filter(rt == -1) %>%
  count()

```
Here we can see that some participant missed some trials.


Let's see how accuracy is coded when response is -1:

```{r}
head(randomDot[randomDot$rt == -1,]$acc)
```

So it is coded as "NA", great. However:

```{r}
nrow(randomDot[is.na(randomDot$acc),]) #total of NA
nrow(randomDot[randomDot$resp == -1,]) # total of timeouts
```

There are more NA's in acc than can be explained by timeouts. This means that also wrong responses are coded as NA. We need to recode those.

```{r}
randomDot[is.na(randomDot$acc),]$acc <- 0 #recode everything that is wrong or timeout as 0
```

### Check the overall accuracy of participants, filtering by timeouts:

```{r timeout table}
aggregate(acc ~ subjID, data = randomDot[!(randomDot$resp == -1),], FUN = mean)# without timeouts
```

Now that we have all tests separated, better to remove this file:
```{r, echo=FALSE}
rm(testing)
```

# Data visualization
Okay, from the sanity checks done above we can draw two conclusions: 

1. Learning and Testing was presented as it was supposed to be and 

2. data was stored correctly


Let's see now if data makes sense.


## Select the version of the experiment

Select the version of the experiment you want:

- Version 14 has 80 subjects, label picture task has 2500ms as timeout

- Version 15 has 42 subjects, label picture task has 3500ms as timeout

```{r}
ver2 <- c(14)
```

## Reaction times

```{r}
rbind(generalizationPL, generalizationLP, contingencyJudgement)-> alltasks
alltasks <- droplevels(alltasks)
```


```{r RT hist}
gghistogram(alltasks,
       x = "rt",
       y = "..count..",
       xlab = "rt", 
       color = "task", 
       fill = "task",
       bins = 40,
       palette = "jco",
       add = "median"
)

```

The two generalization tasks looks quite different. I'm going to plot it separately for a better inspection:

```{r histogram separated by generalization tasks, fig.height = 8, fig.width = 10, fig.align= "center"}
p<- gghistogram(alltasks, #will throw warnings related to non responses but that's okay, ggplot simply removes them
       x = "rt",
       y = "..count..",
       xlab = "rt",
       facet.by = "task",
       add = "median",
       bins = 40
)

facet(p, facet.by = "task",
      nrow = 3,
      ncol = 1)
```

The tails of the first two tasks don't end smoothly, especially in task 2.

## accuracy

### RandomDot

```{r}
unique(randomDot$subjID)-> subj;
randomDot-> randomTask

trials <- c(rep('0', 6), rep('1', 5), 
              rep('2', 5), rep('3', 5), 
              rep('4', 5))

trialstot <- as.factor(rep(trials, length(subj)))

randomTask$blocks <- trialstot
```

#### How many timeouts by participant?

```{r}
randomTask$timeout <- ifelse(randomTask$resp== -1, 1, 0)

```

```{r}
temp<-randomTask %>%
  count(timeout, subjID) %>%
  filter(timeout == 1)

unique(temp$subjID)-> subjs

temp2<-randomTask[!(randomTask$subjID %in% subjs),] %>%
  count(timeout, subjID,  ) %>%
  filter(timeout == 0)

temp2[temp2$timeout==0,]$n <- 0

rbind(temp,temp2)-> timeout
```

Histrogram by participant:

```{r, fig.height = 4, fig.width = 6, fig.align= "center" }

hist(timeout$n, xlab = 'number of timeouts', 
     main = '', 
     col=grey(.80), 
     border=grey(0),
     breaks = seq(0,max(timeout$n),1))
```



```{r}
timeout <- randomTask %>%
  group_by(subjID, blocks) %>%
  filter(resp == -1) %>%
  count() 


ggbarplot(timeout[timeout$n>1 ,], x = "blocks", y = "n",
          facet.by = "subjID",
          sort.by.groups = TRUE,     # Sort inside each group
          ylab = "num of timeouts")
```
#### Subjects that made more than 3 timeouts

```{r}
unique(timeout[timeout$n>3,]$subjID) -> problematicPeople
```


```{r}
accdistr <- randomTask[!(randomTask$resp == -1),] %>%
  group_by(subjID, blocks,  ) %>%
  summarise(m = mean(acc))
```


```{r}

ggstripchart(accdistr, x = "blocks", y = "m",
             xlab = "blocks",
             ylab = "accuracy",
             add = "mean_ci",
             size = 2,
             color = "darkgray",
             shape = 21,
             fill = "gray",
             error.plot = "pointrange",
             add.params = list(color = "black",
                               size = 0.7)) +
  scale_y_continuous(limits = c(0.1, 1), oob = scales::squish) + #to prevent jitter to move above 100%
  geom_hline(yintercept = .50, col='red', lwd=1);

```
```{r}
accdistr <- randomTask[!(randomTask$resp == -1),] %>%
  group_by(subjID, blocks) %>%
  summarise(m = mean(acc))

accdistr[accdistr$m<=.5,]
```



#### People that scored less than 70%:



```{r}
unique(accdistr[accdistr$m<.7,]$subjID) -> dumbPeople
```

```{r}
setdiff(dumbPeople, problematicPeople)-> dumbPeople
```

Let's consider them as bad subjects.
```{r}
c(problematicPeople, dumbPeople)->badsubjs
```


```{r}
rm(temp, temp2, timeout, subj, subjs, trials, trialstot, accdistr)
```

### Task 1: from picture to labels

The column fribbleID stores the fribble presented, while the column label stores the labels presented. Resp column in this task refers to the label selected. Category and frequency refers to the fribbleID column.

I'm going to add 1 in the accuracy column for every instance where response matches the category column, i.e., the participant correctly associated the fribble to its label.

I remove the no-response, and compute accuracy based on category and response.

#### How many participants do we have per learning?
```{r check number of participants}
length(unique(generalizationPL$subjID))
fl<- length(unique(generalizationPL[generalizationPL$learning=='FL' & 
                                      !(generalizationPL$subjID %in% badsubjs),]$subjID))
lf<- length(unique(generalizationPL[generalizationPL$learning=='LF' &
                                      !(generalizationPL$subjID %in% badsubjs),]$subjID))
fl
lf
```

We have `r fl` for feature-label learning, and `r lf` for label-feature learning.


#### Check tails of the rt distribution

The point is that we can't rely on responses made very early, because these might be simply mistakes or technical errors.

```{r hist of rt}
par(mfrow=c(1,2))
hist(generalizationPL[generalizationPL$rt<600 & 
                        !(generalizationPL$subjID %in% badsubjs),]$rt, main = 'rt < 600ms', xlab = 'trials');

hist(generalizationPL[generalizationPL$rt>2000 & 
                        !(generalizationPL$subjID %in% badsubjs),]$rt, main = 'rt > 2000ms', xlab = 'trials');
par(mfrow=c(1,1))

```

I would remove rt <100ms for all tasks.

#### How many, what type of trials do we have?
```{r}
round(nrow(generalizationPL[generalizationPL$rt<100 & 
                              !(generalizationPL$subjID %in% badsubjs),]) / nrow(generalizationPL[!(generalizationPL$subjID %in% badsubjs),])*100,2)
```


```{r  accuracy}
rm(fl,lf)
pictureLabel <- generalizationPL[!(is.na(generalizationPL$resp)),]

pictureLabel$acc <- 0;
pictureLabel[pictureLabel$category==1 & pictureLabel$resp=='dep',]$acc <- 1;

pictureLabel[pictureLabel$category==2 & pictureLabel$resp=='bim',]$acc <- 1;

pictureLabel[pictureLabel$category==3 & pictureLabel$resp=='tob',]$acc <- 1;

```

```{r check missing data}
n <- length(unique(pictureLabel[!(pictureLabel$subjID %in% badsubjs),]$subjID))

nrows <- (nrow(generalizationPL[!(generalizationPL$subjID %in% badsubjs),])) - (nrow(pictureLabel[!(pictureLabel$subjID %in% badsubjs),]))
```

```{r subjects lost?}
sort(unique(pictureLabel[!(pictureLabel$subjID %in% badsubjs),]$subjID))-> subjs;
sort(unique(generalizationPL[!(generalizationPL$subjID %in% badsubjs),]$subjID)) ->totsubjs;

subjmissed<- setdiff(totsubjs, subjs);

rm(subjs, totsubjs);
```
We have `r n` participants in this task, this is -`r length(subjmissed)` compared to our total number of participants. The subject(s) that didn't answer at all the task is: `r subjmissed`. 
We have lost also `r nrows` responses, that is `r nrows/nrow(generalizationPL)*100` over the total: `r nrow(generalizationPL)`.

How many trials per participant do we have now?

```{r}
pictureLabel %>%
  group_by(subjID) %>%
  count() %>% filter(n<=18)
```
No one had less than 18 trials, over the total (24). That's fine!

#### Barplot accuracy by category + frequency + learning

picture label 

```{r aggregating}
c(badsubjs, subjmissed) -> badsubjs

rm(n, subjmissed, nrows)

ss_prop<-aggregate(acc ~ frequency+category+subjID+learning, 
                   data = pictureLabel[pictureLabel$rt > 100 &
                                       !(pictureLabel$subjID %in% badsubjs),], FUN = mean)
```



Plot aggregated over subjs.
To see accuracy distributed over categories.

```{r}

ms <- ss_prop %>%
  group_by( category, frequency, learning) %>%
  summarise(n=n(),
    mean=mean(acc),
    sd=sd(acc)
  ) %>%
  mutate( se=sd/sqrt(n))  %>% 
  mutate( ci=se * qt((1-0.05)/2 + .5, n-1))

ms$frequency <- as.factor(ms$frequency)
plyr::revalue(ms$frequency, c("25"="low"))-> ms$frequency;
plyr::revalue(ms$frequency, c("75"="high"))-> ms$frequency;

ggplot(aes(x = category, y = mean, fill = frequency), data = ms) +
  facet_grid( . ~ learning) + 
  geom_bar(stat = "identity", color='white', position=position_dodge(), size=1.2) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.15, size=1,position=position_dodge(.9)) +
  ylab("Accuracy ") +
  xlab("category") +
  ggtitle('pictureLabels') +
  coord_cartesian(ylim = c(0, 1))+
  ggpubr::theme_pubclean() + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(text = element_text(size=10)) +
  geom_hline(yintercept = .33, col='red', lwd=1);

```

#### Violin plot accuracy by category + frequency + learning

```{r, fig.height = 5, fig.width = 8, fig.align= "center" }
df <- aggregate(acc ~ subjID+frequency+learning+category, 
                data = pictureLabel[pictureLabel$rt > 100  &
                                       !(pictureLabel$subjID %in% badsubjs),], mean)
df$frequency <- as.factor(df$frequency)
plyr::revalue(df$frequency, c("25"="low"))-> df$frequency;
plyr::revalue(df$frequency, c("75"="high"))-> df$frequency;

ggviolin(df, x = "frequency", y = "acc", fill = "frequency",
         palette = c("#00AFBB", "#E7B800"),
         add = "boxplot", 
         add.params = list(fill = "white"),
         trim=TRUE) +
        ggtitle('pictureLabels') +
        facet_grid( learning ~ category) +
        theme_pubclean()+
  geom_hline(yintercept = .33, col='red', lwd=1);

```

#### Violin plot accuracy by frequency + learning

Let's see how participants scored for the high/low frequency: 


```{r}
df <- aggregate(acc ~ subjID+frequency+learning, 
                data = pictureLabel[pictureLabel$rt > 100  &
                                       !(pictureLabel$subjID %in% badsubjs),], mean)
df$frequency <- as.factor(df$frequency)
plyr::revalue(df$frequency, c("25"="low"))-> df$frequency;
plyr::revalue(df$frequency, c("75"="high"))-> df$frequency;

ggviolin(df, x = "frequency", y = "acc", fill = "frequency",
         palette = c("#00AFBB", "#E7B800"),
         add = "boxplot", 
         add.params = list(fill = "white"),
         trim=TRUE) +
        ggtitle('pictureLabels') +
        facet_grid( . ~ learning) +
        theme_pubclean()+
  geom_hline(yintercept = .33, col='red', lwd=1);

```

```{r}
df %>%
  group_by(learning, frequency) %>%
  summarise(mean(acc))
```

Closer inspection:

```{r}
par(mfrow=c(2,2))
hist(df[df$frequency=='low' & df$learning=='FL' ,]$acc, xlab = 'acc', main = 'low freq - FL ')
hist(df[df$frequency=='low' & df$learning=='LF',]$acc, xlab = 'acc', main = 'low freq - LF ')
hist(df[df$frequency=='high' & df$learning=='FL',]$acc, xlab = 'acc', main = 'high freq - FL ')
hist(df[df$frequency=='high' & df$learning=='LF',]$acc, xlab = 'acc', main = 'high freq - LF ')
par(mfrow=c(1,1))

```

#### Barplot accuracy by frequency + learning

```{r, include=TRUE}
#barPlot aggregated over categories:

ms <- aggregate(acc ~ subjID+frequency+learning, 
                data=pictureLabel[pictureLabel$rt > 100 &
                                  !(pictureLabel$subjID %in% badsubjs)  ,], FUN= mean)

df<- ms %>%
  group_by(frequency, learning)%>%
  summarise(
    mean = mean(acc),
    sd = sd(acc),
    n = n()) %>%
  mutate( se=sd/sqrt(n))  %>% 
  mutate( ci=se * qt((1-0.05)/2 + .5, n-1))

df$frequency <- as.factor(df$frequency)
plyr::revalue(df$frequency, c("25"="low"))-> df$frequency;
plyr::revalue(df$frequency, c("75"="high"))-> df$frequency;


pl<-ggplot(aes(x = frequency, y = mean, fill = frequency), data = df) +
  facet_grid( . ~ learning) +
  geom_bar(stat = "identity", color='white', position=position_dodge(), size=1.2) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.15, size=1,position=position_dodge(.9)) +
  ylab("Accuracy ") +
  xlab("frequency") +
  ggtitle('pictureLabels') +
  coord_cartesian(ylim = c(0, 1))+
  ggpubr::theme_pubclean() + 
  theme(legend.position = "none") +
  theme(text = element_text(size=10)) +
  geom_hline(yintercept = .33, col='red', lwd=1);

```

### Task 2: from label to pictures

Let's check now the generalizaton from label to pictures:

```{r}
length(unique(generalizationLP$subjID))
fl<- length(unique(generalizationLP[generalizationLP$learning=='FL' & 
                                      !(generalizationLP$subjID %in% badsubjs),]$subjID))

lf<- length(unique(generalizationLP[generalizationLP$learning=='LF' &
                                      !(generalizationLP$subjID %in% badsubjs),]$subjID))
fl
lf
```


#### How many participants do we have per learning?

We have `r fl` for feature-label learning, and `r lf` for label-feature learning.

```{r}
rm(fl,lf)
labelPicture <- generalizationLP[!(is.na(generalizationLP$resp)),]
n<- length(unique(labelPicture$subjID))
nrows <- (nrow(generalizationLP)) - (nrow(labelPicture))

sort(unique(labelPicture$subjID))-> subjs;
sort(unique(generalizationLP$subjID)) ->totsubjs;

subjmissed<- setdiff(totsubjs, subjs);

```
Great, we have `r n` participants in this task, so -`r length(subjmissed)`, and we have missed `r nrows` over the total `r nrow(generalizationLP)`, that is `r nrows/nrow(generalizationLP)*100`. The subject(s) that missed completely the task is: `r subjmissed`.

#### How many, what type of trials do we have?

How many datapoints did we lose for no-responses?
```{r}
round(nrow(generalizationLP[(is.na(generalizationLP$resp)) &
                              !(generalizationLP$subjID %in% badsubjs),]) /nrow(generalizationLP[!(generalizationLP$subjID %in% badsubjs),])*100,2)
```

How many trials were rt < 100?
```{r}
round(nrow(generalizationLP[generalizationLP$rt<100 & 
                              !(generalizationLP$subjID %in% badsubjs),])/ nrow(generalizationLP[!(generalizationLP$subjID %in% badsubjs),])*100,2)
```

Once trimmed, how many trials per participant do we have in this task?
```{r}
labelPicture %>%
  group_by(subjID) %>%
  count() %>%
  filter(n<=18)
```

Here we have less datapoints. 
For sure, 1422680 needs to be added to the black list because has few correct trials.

```{r}
c(badsubjs, 1422680, 1432009) -> badsubjs
```



#### Check tails of the rt distribution

```{r hist rt tails, fig.height = 4, fig.width = 6, fig.align= "center" }
par(mfrow=c(1,2))
hist(generalizationLP[generalizationLP$rt<600 & 
                        !(generalizationLP$subjID %in% badsubjs),]$rt, main = 'rt < 600ms', xlab = 'trials');
hist(generalizationLP[generalizationLP$rt>2000 & 
                        !(generalizationLP$subjID %in% badsubjs),]$rt, main = 'rt > 2000ms', xlab = 'trials');
par(mfrow=c(1,1))
```




```{r acc calculation}
rm(n, nrows, subjs, totsubjs);
labelPicture$acc <- 0;
labelPicture[labelPicture$category==1 & labelPicture$label=='dep',]$acc <- 1;
labelPicture[labelPicture$category==2 & labelPicture$label=='bim',]$acc <- 1;
labelPicture[labelPicture$category==3 & labelPicture$label=='tob',]$acc <- 1;

```


#### Barplot accuracy by category+learning+frequency

Calculate the proportion of correct in each condition



```{r}
rm(subjmissed)
ss_prop<-aggregate(acc ~ frequency+category+subjID+learning, 
                   data = labelPicture[labelPicture$rt > 100 &
                                         labelPicture$rt <= 2500 &
                                         !(labelPicture$subjID %in% badsubjs),], FUN = mean)

```


Plot aggregated over subjs.
To see accuracy distributed over categories.

```{r}

ms <- ss_prop %>%
  group_by(category, frequency, learning) %>%
  summarise(
    n=n(),
    mean=mean(acc),
    sd=sd(acc)
  ) %>%
  mutate( se=sd/sqrt(n))  %>% 
  mutate( ci=se * qt((1-0.05)/2 + .5, n-1))

ms$frequency <- as.factor(ms$frequency)
plyr::revalue(ms$frequency, c("25"="low"))-> ms$frequency;
plyr::revalue(ms$frequency, c("75"="high"))-> ms$frequency;

ggplot(aes(x = category, y = mean, fill = frequency), data = ms) +
  facet_grid( . ~ learning) + 
  geom_bar(stat = "identity", color='white', position=position_dodge(), size=1.2) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.15, size=1,position=position_dodge(.9)) +
  ylab("Accuracy ") +
  xlab("category") +
  ggtitle('labelPictures') +
  coord_cartesian(ylim = c(0, 1))+
  ggpubr::theme_pubclean() + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(text = element_text(size=10)) +
  geom_hline(yintercept = .33, col='red', lwd=1);

```

#### Violin plot accuracy by category+learning+frequency

```{r, fig.height = 4, fig.width = 6, fig.align= "center" }
ms <- aggregate(acc ~ subjID+frequency+learning+category, 
                data = labelPicture[labelPicture$rt > 100 & 
                                      labelPicture$rt <=2500 &
                                         !(labelPicture$subjID %in% badsubjs),], mean)

ms$frequency <- as.factor(ms$frequency)
plyr::revalue(ms$frequency, c("25"="low"))-> ms$frequency;
plyr::revalue(ms$frequency, c("75"="high"))-> ms$frequency;

ggviolin(ms, x = "frequency", y = "acc", fill = "frequency",
         palette = c("#00AFBB", "#E7B800"),
         add = "boxplot", 
         add.params = list(fill = "white"),
         trim=TRUE) +
         ggtitle('labelPictures') +
        facet_grid( learning ~ category) +
        theme_pubclean()+
  geom_hline(yintercept = .33, col='red', lwd=1);
#rm(ms, ss_prop)

```

#### Violinplot accuracy by learning+frequency


```{r, fig.height = 4, fig.width = 6, fig.align= "center" }
ms <- aggregate(acc ~ subjID+frequency+learning, 
                data = labelPicture[labelPicture$rt > 100 &
                                      labelPicture$rt <= 2500 &
                                         !(labelPicture$subjID %in% badsubjs),], mean)

ms$frequency <- as.factor(ms$frequency)
plyr::revalue(ms$frequency, c("25"="low"))-> ms$frequency;
plyr::revalue(ms$frequency, c("75"="high"))-> ms$frequency;

ggviolin(ms, x = "frequency", y = "acc", fill = "frequency",
         palette = c("#00AFBB", "#E7B800"),
         add = "boxplot", 
         add.params = list(fill = "white"),
         trim=TRUE) +
         ggtitle('labelPictures') +
        facet_grid( . ~ learning) +
        theme_pubclean()+
  geom_hline(yintercept = .33, col='red', lwd=1);
#rm(ms, ss_prop)


```

```{r}
ms %>%
  group_by(learning, frequency) %>%
  summarise(mean(acc))
```


```{r hist distribution of responses}
par(mfrow=c(2,2))
hist(ms[ms$frequency=='low' & ms$learning=='FL',]$acc, xlab = 'acc', main = 'low freq - FL ')
hist(ms[ms$frequency=='low' & ms$learning=='LF',]$acc, xlab = 'acc', main = 'low freq - LF ')
hist(ms[ms$frequency=='high' & ms$learning=='FL',]$acc, xlab = 'acc', main = 'high freq - FL ')
hist(ms[ms$frequency=='high' & ms$learning=='LF',]$acc, xlab = 'acc', main = 'high freq - LF ')
par(mfrow=c(1,1))

```


#### Barplot accuracy by frequency + learning

```{r, include=TRUE}
#barPlot aggregated over categories:

ms <- aggregate(acc ~ subjID+frequency+learning, 
                data=labelPicture[labelPicture$rt > 100 &
                                    labelPicture$rt <= 2500 &
                                  !(labelPicture$subjID %in% badsubjs)  ,], FUN= mean)

df<- ms %>%
  group_by(frequency, learning)%>%
  summarise(
    mean = mean(acc),
    sd = sd(acc),
    n = n()) %>%
  mutate( se=sd/sqrt(n))  %>% 
  mutate( ci=se * qt((1-0.05)/2 + .5, n-1))

df$frequency <- as.factor(df$frequency)
plyr::revalue(df$frequency, c("25"="low"))-> df$frequency;
plyr::revalue(df$frequency, c("75"="high"))-> df$frequency;


lp<-ggplot(aes(x = frequency, y = mean, fill = frequency), data = df) +
  facet_grid( . ~ learning) +
  geom_bar(stat = "identity", color='white', position=position_dodge(), size=1.2) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.15, size=1,position=position_dodge(.9)) +
  ylab("Accuracy ") +
  xlab("frequency") +
  ggtitle('labelPictures') +
  coord_cartesian(ylim = c(0, 1))+
  ggpubr::theme_pubclean() + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(text = element_text(size=10)) +
  geom_hline(yintercept = .33, col='red', lwd=1);

```


### Comparison by frequency by learning by tasks
Quick summary of what we have so far:

```{r barplots with both tasks, fig.height = 12, fig.width = 8, fig.align= "center" }
grid.arrange(pl,lp)
```



What's going on in the low frequency condition? 
One way to see whether they simply learned another association is to check that wrong choices are distributed equally (50%) to the other two categories. If they are, then they didn't learn anything, but if they are not distributed equally, they have learned another association.

Label picture: 
```{r}
#select only inaccurate trials
temp <- labelPicture[labelPicture$acc==0,] 

round(nrow(temp)/nrow(labelPicture)*100,2)
```

How many of those are low frequency trials?

```{r}
round(nrow(temp[temp$frequency==25,])/nrow(labelPicture)*100,2)
```

How many of those are low frequency trials and how are they distributed across learnings?
 
```{r}
round(nrow(temp[temp$frequency==25 & temp$learning=="FL",])/nrow(labelPicture)*100,2)
round(nrow(temp[temp$frequency==25 & temp$learning=="LF",])/nrow(labelPicture)*100,2)

```

FL people make more errors in the low freq condition

How many of those are high frequency trials and how are they distributed across learnings?

```{r}
round(nrow(temp[temp$frequency==75 & temp$learning=="FL",])/nrow(labelPicture)*100,2)
round(nrow(temp[temp$frequency==75 & temp$learning=="LF",])/nrow(labelPicture)*100,2)

```
While they are pretty much the same in the high frequency


Label picture task:

correct choice is listed in "label", that is, label presented.
Participant's choice is listed in "category", that is, the fribble's category.

```{r}
temp %>%
  filter(frequency=="25") %>%
  group_by(learning, label, category) %>%
  count()
```

Nope, they definitely learned another association. The association they have learned is based on the high saliency feature, rather than on the low saliency one. Let's see if that is the case also for the other task:

Picture label task: 
```{r}
#select only inaccurate trials
temp <- pictureLabel[pictureLabel$acc==0,] 

round(nrow(temp)/nrow(pictureLabel)*100,2)

```

How many of those are low frequency trials?

```{r}
round(nrow(temp[temp$frequency==25,])/nrow(pictureLabel)*100,2)
```

How many of those are low frequency trials and how are they distributed across learnings?

```{r}
round(nrow(temp[temp$frequency==25 & temp$learning=="FL",])/nrow(pictureLabel)*100,2)

round(nrow(temp[temp$frequency==25 & temp$learning=="LF",])/nrow(pictureLabel)*100,2)
```

How many of those are high frequency trials and how are they distributed across learnings?

```{r}
round(nrow(temp[temp$frequency==75 & temp$learning=="FL",])/nrow(pictureLabel)*100,2)

round(nrow(temp[temp$frequency==75 & temp$learning=="LF",])/nrow(pictureLabel)*100,2)
```

Picture label task:

correct choice is listed in "category", that is, the category of the fribble presented.
Participant's choice is listed in "resp" column, that is, the label chosen.

```{r}
temp %>%
  filter(frequency=="25") %>%
  group_by(learning, category, resp) %>%
  count()
```

In both tasks participants were driven by the high salient feature in making errors, they simply learned only one association between the label and the high salient feature, and made decisions based on this.

## Speed-accuracy trade-off by tasks

Inspection of the speed-accuracy trade-off:

Label Picture
```{r}
aggregate(acc ~ subjID+learning, labelPicture[labelPicture$rt > 100 &
                                         !(labelPicture$subjID %in% badsubjs) ,], mean)-> speedacc

aggregate(rt ~ subjID+learning, labelPicture[labelPicture$rt > 100 &
                                         !(labelPicture$subjID %in% badsubjs),], mean)-> speedacc2
merge(speedacc, speedacc2, by =  c("subjID", "learning"))-> speedacc

ggplot(aes(x=rt, y=acc), 
           data = speedacc) + 
  facet_grid( . ~ learning) + 
  geom_point( shape = 21, fill = "white", size = 3, stroke = 1.5) +
  #geom_smooth(method = "lm", formula = y ~ poly(x,2), se = TRUE, color = "#0892d0", fill = "lightgray") +
  geom_hline(yintercept = 0.33, lty = "dashed", color = 'red') +
  coord_cartesian(ylim = c(0, 1))+
  ggthemes::theme_hc()+
  xlab("Average RT on subjs") +
  ylab("Proportion Correct") +
  ggtitle("speed-acc tradeoff - labelPicture")

```

```{r, fig.height = 8, fig.width = 12, fig.align= "center"}
aggregate(acc ~ subjID+learning+frequency, labelPicture[labelPicture$rt > 100 &
                                                          labelPicture$rt <= 2500 &
                                         !(labelPicture$subjID %in% badsubjs) ,], mean)-> speedacc

aggregate(rt ~ subjID+learning+frequency, labelPicture[labelPicture$rt > 100 &
                                                         labelPicture$rt <= 2500 &
                                         !(labelPicture$subjID %in% badsubjs),], mean)-> speedacc2
merge(speedacc, speedacc2, by =  c("subjID", "learning", "frequency"))-> speedacc
rm(speedacc2)
dplyr::recode(speedacc$frequency, "25"="low", "75"="high")-> speedacc$frequency;

ggplot(aes(x=rt, y=acc), 
           data = speedacc) + 
  facet_grid( learning ~ frequency) + 
  geom_point( shape = 21, fill = "white", size = 3, stroke = 1.5) +
  #geom_smooth(method = "lm", formula = y ~ poly(x,2), se = TRUE, color = "#0892d0", fill = "lightgray") +
  geom_hline(yintercept = 0.33, lty = "dashed", color = 'red') +
  coord_cartesian(ylim = c(0, 1))+
  ggthemes::theme_base()+
  xlab("Average RT on subjs") +
  ylab("Proportion Correct") +
  ggtitle("speed-acc tradeoff - labelPicture")

```

```{r}
speedacc %>%
  group_by(frequency, learning) %>%
  summarise(mean(rt), median(rt))
```



PictureLabel

```{r}
aggregate(acc ~ subjID+learning, pictureLabel[pictureLabel$rt > 100  &
                                         !(pictureLabel$subjID %in% badsubjs),], mean)-> speedacc

aggregate(rt ~ subjID+learning, pictureLabel[pictureLabel$rt > 100  &
                                         !(pictureLabel$subjID %in% badsubjs),], mean)-> speedacc2
merge(speedacc, speedacc2, by =  c("subjID", "learning"))-> speedacc

ggplot(aes(x=rt, y=acc), 
           data = speedacc) + 
  facet_grid( . ~ learning) + 
  geom_point( shape = 21, fill = "white", size = 3, stroke = 1.5) +
  #geom_smooth(method = "lm", formula = y ~ poly(x,2), se = TRUE, color = "#0892d0", fill = "lightgray") +
  geom_hline(yintercept = 0.33, lty = "dashed", color = 'red') +
  coord_cartesian(ylim = c(0, 1))+
  ggthemes::theme_hc()+
  xlab("Average RT on subjs") +
  ylab("Proportion Correct") +
  ggtitle("speed-acc tradeoff - pictureLabel")

```

```{r, fig.height = 8, fig.width = 12, fig.align= "center"}
aggregate(acc ~ subjID+learning+frequency, pictureLabel[pictureLabel$rt > 100  &
                                         !(pictureLabel$subjID %in% badsubjs),], mean)-> speedacc
aggregate(rt ~ subjID+learning+frequency, pictureLabel[pictureLabel$rt > 100  &
                                         !(pictureLabel$subjID %in% badsubjs),], mean)-> speedacc2
merge(speedacc, speedacc2, by =  c("subjID", "learning", "frequency"))-> speedacc
rm(speedacc2)
dplyr::recode(speedacc$frequency, "25"="low", "75"="high")-> speedacc$frequency;

ggplot(aes(x=rt, y=acc), 
           data = speedacc) + 
  facet_grid( learning ~ frequency) + 
  geom_point( shape = 21, fill = "white", size = 3, stroke = 1.5) +
  #geom_smooth(method = "lm", formula = y ~ poly(x,2), se = TRUE, color = "#0892d0", fill = "lightgray") +
  geom_hline(yintercept = 0.33, lty = "dashed", color = 'red') +
  coord_cartesian(ylim = c(0, 1))+
  ggthemes::theme_base()+
  xlab("Average RT on subjs") +
  ylab("Proportion Correct") +
  ggtitle("speed-acc tradeoff - pictureLabel")


```

```{r}
speedacc %>%
  group_by(frequency, learning) %>%
  summarise(mean(rt), median(rt))
```

## Final Comparisons 

Barplot labelPicture

```{r, fig.align='center'}

ms <- aggregate(acc ~ subjID+frequency+learning, 
                data=labelPicture[labelPicture$rt > 100  &
                                         !(labelPicture$subjID %in% badsubjs),], FUN= mean)

df<- ms %>%
  group_by(frequency, learning)%>%
  summarise(
    mean = mean(acc),
    sd = sd(acc),
    n = n()) %>%
  mutate( se=sd/sqrt(n))  %>% 
  mutate( ci=se * qt((1-0.05)/2 + .5, n-1))

df$frequency <- as.factor(df$frequency)
plyr::revalue(df$frequency, c("25"="low"))-> df$frequency;
plyr::revalue(df$frequency, c("75"="high"))-> df$frequency;


lp<-ggplot(aes(x = frequency, y = mean, fill = frequency), data = df) +
  facet_grid( . ~ learning) +
  geom_bar(stat = "identity", color='white', position=position_dodge(), size=1.2) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.15, size=1,position=position_dodge(.9)) +
  ylab("Accuracy ") +
  xlab("frequency") +
  ggtitle("labelPictures") +
  coord_cartesian(ylim = c(0, 1))+
  ggpubr::theme_pubclean() + 
  theme(legend.position = "none")  +
  theme(text = element_text(size=10)) +
  geom_hline(yintercept = .33, col='red', lwd=1);

```

```{r}
grid.arrange(lp, pl, ncol=2)

```

```{r, fig.height = 4, fig.width = 9, fig.align='center'}

ms <- aggregate(acc ~ subjID+frequency+learning, 
                data=labelPicture[labelPicture$rt > 100  &
                                    labelPicture$rt <=2500 &
                                         !(labelPicture$subjID %in% badsubjs),], FUN= mean)

ms$frequency <- as.factor(ms$frequency)
plyr::revalue(ms$frequency, c("25"="low"))-> ms$frequency;
plyr::revalue(ms$frequency, c("75"="high"))-> ms$frequency;

lp_violin<- ggviolin(ms, x = "frequency", y = "acc", fill = "frequency",
         palette = c("#00AFBB", "#E7B800"),
         add = "boxplot", 
         add.params = list(fill = "white"),
         trim=TRUE) +
         ggtitle('labelPictures') +
        facet_grid( . ~ learning) +
        theme_pubclean()+
  theme(legend.position = "none") +
  geom_hline(yintercept = .33, col='red', lwd=1);


ms <- aggregate(acc ~ subjID+frequency+learning, 
                data=pictureLabel[pictureLabel$rt > 100  &
                                         !(pictureLabel$subjID %in% badsubjs),], FUN= mean)

ms$frequency <- as.factor(ms$frequency)
plyr::revalue(ms$frequency, c("25"="low"))-> ms$frequency;
plyr::revalue(ms$frequency, c("75"="high"))-> ms$frequency;

pl_violin<- ggviolin(ms, x = "frequency", y = "acc", fill = "frequency",
         palette = c("#00AFBB", "#E7B800"),
         add = "boxplot", 
         add.params = list(fill = "white"),
         trim=TRUE) +
         ggtitle('pictureLabel') +
        facet_grid( . ~ learning) +
        theme_pubclean()+
  theme(legend.position = "none") +
  geom_hline(yintercept = .33, col='red', lwd=1);

grid.arrange(lp_violin, pl_violin, ncol=2)

#rm(ms, ss_prop)
```



Barplots + violinPlots with data from both tasks:

```{r}
rm(ms, df, ss_prop)
genTask <- rbind(labelPicture, pictureLabel)
```

```{r}
ms <- aggregate(acc ~ subjID+frequency+learning, data = genTask[genTask$rt>100 &
                                                                  genTask$Experiment.Version==ver2 &
                                         !(genTask$subjID %in% badsubjs),], mean)

ms$frequency <- as.factor(ms$frequency)
plyr::revalue(ms$frequency, c("25"="low"))-> ms$frequency;
plyr::revalue(ms$frequency, c("75"="high"))-> ms$frequency;

ggviolin(ms, x = "frequency", y = "acc", fill = "frequency",
         palette = c("#00AFBB", "#E7B800"),
         add = "boxplot", 
         add.params = list(fill = "white"),
         trim=TRUE) +
        ggtitle('labelPictures + pictureLabels') + 
        facet_grid( . ~ learning) +
        theme_pubclean()+
  geom_hline(yintercept = .33, col='red', lwd=1);

```

```{r}
ms <- aggregate(acc ~ subjID+frequency+learning, data=genTask[genTask$rt>100 &
                                                                genTask$Experiment.Version==ver2 &
                                         !(genTask$subjID %in% badsubjs),], FUN= mean)

df<- ms %>%
  group_by(frequency, learning)%>%
  summarise(
    mean = mean(acc),
    sd = sd(acc),
    n = n()) %>%
  mutate( se=sd/sqrt(n))  %>% 
  mutate( ci=se * qt((1-0.05)/2 + .5, n-1))

df$frequency <- as.factor(df$frequency)
plyr::revalue(df$frequency, c("25"="low"))-> df$frequency;
plyr::revalue(df$frequency, c("75"="high"))-> df$frequency;


ggplot(aes(x = frequency, y = mean, fill = frequency), data = df) +
  facet_grid( . ~ learning) +
  geom_bar(stat = "identity", color='white', position=position_dodge(), size=1.2) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.15, size=1,position=position_dodge(.9)) +
  ylab("Accuracy ") +
  xlab("frequency") +
  ggtitle("labelPicture") +
  ggtitle('picturelabels + labelpictures') +
  coord_cartesian(ylim = c(0, 1))+
  ggpubr::theme_pubclean() + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(text = element_text(size=10)) +
  geom_hline(yintercept = .33, col='red', lwd=1);

```

# Task 3: Contingency judgement 

```{r}
length(unique(contingencyJudgement$subjID))
fl<- length(unique(contingencyJudgement[contingencyJudgement$learning=='FL' & contingencyJudgement$Experiment.Version==ver2,]$subjID))

lf<- length(unique(contingencyJudgement[contingencyJudgement$learning=='LF' & contingencyJudgement$Experiment.Version==ver2,]$subjID))

fl
lf
```


We have `r fl` for feature-label learning, and `r lf` for label-feature learning.

```{r}
rm(fl,lf)
conjudge <- contingencyJudgement[!(is.na(contingencyJudgement$resp)),]
n<- length(unique(conjudge$subjID))
nrows <- (nrow(contingencyJudgement)) - (nrow(conjudge))

sort(unique(conjudge$subjID))-> subjs;
sort(unique(contingencyJudgement$subjID)) ->totsubjs;

subjmissed<- setdiff(totsubjs, subjs);

badsubjs <- c(badsubjs, subjmissed)
badsubjs <- unique(badsubjs)
```
We have `r n` participants in this task, so -`r length(subjmissed)`, and we have missed `r nrows` over the total `r nrow(generalizationLP)`, that is `r nrows/nrow(generalizationLP)*100`. The subject(s) that missed completely the task is/are: `r subjmissed`.



```{r }
par(mfrow=c(1,2))
hist(conjudge[conjudge$rt<1500 & !(conjudge$subjID %in% badsubjs),]$rt, main = 'rt < 1500ms', xlab = 'trials');
hist(conjudge[conjudge$rt>3000 & !(conjudge$subjID %in% badsubjs),]$rt, main = 'rt > 3000ms', xlab = 'trials');
par(mfrow=c(1,1))
```

Resp is coded as factor, need to correct this:

```{r}
as.numeric(levels(conjudge$resp))[conjudge$resp]-> conjudge$resp
```


```{r histogram}
hist(conjudge[!(conjudge$subjID %in% badsubjs),]$resp, main = 'resp distribution', xlab = 'choices')
```

Ok, here we don't have right or wrong answers, but we are more interested in take a look how the participants rated the fribble label association:

```{r}
aggregate(resp ~ category, data = conjudge[!(conjudge$subjID %in% badsubjs),], FUN = mean)
```

Okay, in this task one fribble was presented along with a label. The association between the fribble presented and the label could have been correct, or wrong. In this case then accuracy column does **not** refer to the participants' accuracy, but rather to the fribble-label pair presented. This should be therefore necessarily equal to the chance level, i.e, around 33%, of course this number is dependent by the number of datapoints left without no-responses because we filtered out those.

```{r acc calculation conjudge}
conjudge$acc <- 0;
conjudge[conjudge$category==1 & conjudge$label=='dep',]$acc <- 1;
conjudge[conjudge$category==2 & conjudge$label=='bim',]$acc <- 1;
conjudge[conjudge$category==3 & conjudge$label=='tob',]$acc <- 1;
```

```{r}
mean(conjudge[!(conjudge$subjID %in% badsubjs),]$acc)
```

Quite there, everything good.

```{r}
respDistr<-summarySEwithin(data = conjudge[!(conjudge$subjID %in% badsubjs),], measurevar = "resp", betweenvars = "learning", withinvars = c("frequency", "category", "label"), idvar = "subjID", conf.interval = .95)
respDistr
```

## plot mean responses
```{r, fig.height = 5, fig.width = 8, fig.align= "center"}
plyr::revalue(respDistr$frequency, c("25"="low"))-> respDistr$frequency;
plyr::revalue(respDistr$frequency, c("75"="high"))-> respDistr$frequency;

lollipop<-ggdotchart(respDistr, x = "label", y = "resp",
           color = "category",                                # Color by groups
           palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Custom color palette
           add = "segments",                             # Add segments from y = 0 to dots
           rotate = T,
           add.params = list(color = "lightgray", size = 2), # Change segment color and size
           group = "category",                                # Order by groups
           dot.size = 10,                                 # Large dot size
           label = round(respDistr$resp,1),                        # Add mpg values as dot labels
           font.label = list(color = "white", size = 9, 
                             vjust = 0.5),               # Adjust label parameters
           ggtheme = theme_pubr()                        # ggplot2 theme
           )+ facet_grid( frequency ~ learning) +
  geom_hline(yintercept = 0, linetype = 2, color = "lightgray") 

lollipop
```


Plot to compare with RW weights:

```{r}
plyr::revalue(as.factor(conjudge$frequency), c("25"="low"))-> conjudge$frequency;
plyr::revalue(as.factor(conjudge$frequency), c("75"="high"))-> conjudge$frequency;

```


For each learning condition, look at their average score for each of the 6 combinations of frequency and type:

High frequency:
```{r high freq schema}
highFreqFL<-data.frame(
           learning = rep("FL",9),
           frequency = rep("high",9),
           type = c(rep("match",3), 
                    rep("mismatch-type1",3), 
                    rep("mismatch-type2",3)),
           label = c("dep_cat1", "bim_cat2", "tob_cat3"),
           fribble = c(1.1,2.1,3.1,
                       3.1,1.1,2.1,
                       2.1,3.1,1.1),
           fribbleCategory = c("cat1", "cat2", "cat3", #match
                        "cat3", "cat1", "cat2", #mis-type1
                        "cat2", "cat3", "cat1")) #mis-type2

highFreqLF<-data.frame(
           learning = rep("LF",9),
           frequency = rep("high",9),
           type = c(rep("match",3), 
                    rep("mismatch-type1",3), 
                    rep("mismatch-type2",3)),
           label = c("dep_cat1", "bim_cat2", "tob_cat3"),
           fribble = c(1.1,2.1,3.1,
                       3.1,1.1,2.1,
                       2.1,3.1,1.1),
           fribbleCategory = c("cat1", "cat2", "cat3", #match
                        "cat3", "cat1", "cat2", #mis-type1
                        "cat2", "cat3", "cat1")) #mis-type2

rbind(highFreqFL, highFreqLF)-> highFreq
rm(highFreqFL, highFreqLF)
```

Okay, let's fill each row:

```{r}
resp <- c(
#-----------------------------------------FL LEARNING
  # ROW 1                                              #MATCH
  mean(conjudge[conjudge$learning=="FL" & 
           conjudge$frequency=="high" & 
           conjudge$label=='dep' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 2
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='bim' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 3
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='tob' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 4                                              #MISMATCH -TYPE1
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='dep' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 5
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='bim' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 6
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='tob' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 7                                              #MISMATCH -TYPE2
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='dep' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 8
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='bim' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 9
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="high" & 
           conjudge$label=='tob' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
#----------------------------------------------- LF LEARNING 
# ROW 1                                              #MATCH
  mean(conjudge[conjudge$learning=="LF" & 
           conjudge$frequency=="high" & 
           conjudge$label=='dep' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 2
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='bim' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 3
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='tob' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 4                                              #MISMATCH -TYPE1
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='dep' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 5
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='bim' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 6
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='tob' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 7                                              #MISMATCH -TYPE2
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='dep' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 8
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='bim' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 9
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="high" & 
           conjudge$label=='tob' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp)
)
highFreq$resp <- resp

```

```{r}
highFreq
```
          
          
Low frequency:
```{r low freq schema}
lowFreqFL<-data.frame(
           learning = rep("FL",9),
           frequency = rep("low",9),
           type = c(rep("match",3), 
                    rep("mismatch-type1",3), 
                    rep("mismatch-type2",3)),
           label = c("dep_cat1", "bim_cat2", "tob_cat3"),
           fribble = c(1.2,2.2,3.2,
                       2.2,3.2,1.2,
                       3.2,1.2,2.2),
           fribbleCategory = c("cat1", "cat2", "cat3", #match
                               "cat2", "cat3", "cat1", #mis-type1
                               "cat3", "cat1", "cat2")) #mis-type2

lowFreqLF<-data.frame(
           learning = rep("LF",9),
           frequency = rep("low",9),
           type = c(rep("match",3), 
                    rep("mismatch-type1",3), 
                    rep("mismatch-type2",3)),
           label = c("dep_cat1", "bim_cat2", "tob_cat3"),
           fribble = c(1.2,2.2,3.2,
                       2.2,3.2,1.2,
                       3.2,1.2,2.2),
           fribbleCategory = c("cat1", "cat2", "cat3", #match
                               "cat2", "cat3", "cat1", #mis-type1
                               "cat3", "cat1", "cat2")) #mis-type2
lowFreq<- rbind(lowFreqFL, lowFreqLF)
rm(lowFreqFL, lowFreqLF)
```

```{r}
resp <- c(
#-----------------------------------------FL LEARNING
  # ROW 1                                              #MATCH
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='dep' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 2
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='bim' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 3
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='tob' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 4                                              #MISMATCH -TYPE1
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='dep' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 5
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='bim' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 6
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='tob' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 7                                              #MISMATCH -TYPE2
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='dep' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 8
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='bim' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 9
  mean(conjudge[conjudge$learning=="FL" &
           conjudge$frequency=="low" & 
           conjudge$label=='tob' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),

#-----------------------------------------LF LEARNING
  # ROW 1                                              #MATCH
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='dep' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 2
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='bim' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 3
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='tob' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 4                                              #MISMATCH -TYPE1
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='dep' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 5
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='bim' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 6
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='tob' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 7                                              #MISMATCH -TYPE2
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='dep' & 
           conjudge$category==3 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 8
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='bim' & 
           conjudge$category==1 &
           !(conjudge$subjID %in% badsubjs),]$resp),
  # ROW 9
  mean(conjudge[conjudge$learning=="LF" &
           conjudge$frequency=="low" & 
           conjudge$label=='tob' & 
           conjudge$category==2 &
           !(conjudge$subjID %in% badsubjs),]$resp)
)
lowFreq$resp <- resp

```

```{r}
rbind(highFreq, lowFreq)-> humansWeights
humansWeights$learning <- as.factor(humansWeights$learning); humansWeights$frequency <- as.factor(humansWeights$frequency); humansWeights$type <- as.factor(humansWeights$type); humansWeights$label <- as.factor(humansWeights$label); humansWeights$fribbleCategory <- as.factor(humansWeights$fribbleCategory)
rm(highFreq, lowFreq)
summary(humansWeights)

```

```{r}
dataWeight <- aggregate(resp ~ learning + frequency + type, data = humansWeights,FUN = mean)
dataWeight
```



```{r}
lollipopWeight<-ggdotchart(dataWeight, x = "type", y = "resp",
           #color = "learning",                                # Color by groups
           palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Custom color palette
           add = "segments",                             # Add segments from y = 0 to dots
           rotate = T,
           add.params = list(color = "lightgray", size = 2), # Change segment color and size
           #group = "learning",                                # Order by groups
           dot.size = 10,                                 # Large dot size
           label = round(dataWeight$resp,1),                        # Add mpg values as dot labels
           font.label = list(color = "white", size = 9, 
                             vjust = 0.5),               # Adjust label parameters
           ggtheme = theme_pubr()                        # ggplot2 theme
           )+ facet_grid( learning ~ frequency) +
  geom_hline(yintercept = 0, linetype = 2, color = "lightgray") 

lollipopWeight
```
           
Raw means
           
```{r}
respShape<- summarySEwithin(data = conjudge[!(conjudge$subjID %in% badsubjs) & conjudge$acc==1,], measurevar = "resp", betweenvars = "learning", withinvars = c("frequency", "bodyShape"), idvar = "subjID", conf.interval = .95)
respShape
```
Let's clean the global environment:

```{r}
rm(n_bins, rt_range, problematicPeople, frequency, dumbPeople, break_seq, task, temp, timeslice_range, p, ms, n, nrows, subjs, totsubjs ,genTask, fribbleSet, count_range, alltasks, subjmissed, df)
```


# Bayes factor calculation with GLMMs

## Estimates of the betas from the FLO paper

```{r}
#means
highfreq_mean<- mean(88, 98)
lowfreq_mean <- mean(38, 78)

n <- c(32) 

#sd
highfreq_sd <- c(5*sqrt(n))  #Paper has standard errors represented (I guess),
                     #I'm going to transform it back to standard deviations
lowfreq_sd <- c(5*sqrt(n)) #also, they look the same to me from the picture
                           #but low frequency should lead more variability.
```

Main effect of frequency:
```{r}
frequency_beta<- logodds(highfreq_mean) - logodds(lowfreq_mean)
```

Main effect of learning:
```{r}
#mean
LF_mean <- mean(38, 88)
FL_mean <- mean(78, 98)

n <- c(16)

#sd
LF_sd <- c(5*sqrt(n)) #how can be possible that learnings have the same se?
FL_sd <- c(5*sqrt(n))
```

```{r}
learning_beta <- logodds(FL_mean) - logodds(LF_mean)
#positive > higher in the FL
```

Interaction between freq and learning:

Frequency effect (high-low) is greater in the LF than in FL:
```{r}
#(logodds(highfreq_FL)-logodds(lowfreq_FL))- (logodds(highfreq_LF)-logodds(lowfreq_LF))
freqBylearning_beta <- (logodds(98)-logodds(78))- (logodds(88)-logodds(38))*-1
```

## GLMMs with all tests separately

Picture label

```{r}
pictureLabel$frequency <- as.factor(pictureLabel$frequency)
plyr::revalue(pictureLabel$frequency, c("25"="low"))-> pictureLabel$frequency;
plyr::revalue(pictureLabel$frequency, c("75"="high"))-> pictureLabel$frequency;

pictureLabel$learning = relevel(pictureLabel$learning, ref = "LF")
pictureLabel$frequency = relevel(pictureLabel$frequency, ref = "low")
pictureLabel <- lizCenter(pictureLabel, list("learning" , "frequency", "task"))

```

```{r}
summarySEwithin(data = pictureLabel[pictureLabel$rt > 100 & !(pictureLabel$subjID %in% badsubjs),], measurevar = "acc", betweenvars = "learning", withinvars = "frequency", idvar = "subjID", conf.interval = .95)
```


```{r}
piclab_model <- glmer(acc ~  frequency*learning + (frequency|subjID), 
         data = pictureLabel[pictureLabel$rt > 100 & !(pictureLabel$subjID %in% badsubjs),], 
         family="binomial",
         control=glmerControl(optimizer = "bobyqa"))

adjusted.piclab_model = adjust_intercept_model(piclab_model, chance = log(0.33/(1-0.33)))
round(adjusted.piclab_model,5)

```

```{r}
piclab_model.emm <- emmeans(piclab_model , ~ frequency* learning )
contrast(piclab_model.emm, "consec",  simple = "each", combine = F, adjust = "bonferroni")

```

Label picture

```{r}
labelPicture$frequency <- as.factor(labelPicture$frequency)
plyr::revalue(labelPicture$frequency, c("25"="low"))-> labelPicture$frequency;
plyr::revalue(labelPicture$frequency, c("75"="high"))-> labelPicture$frequency;

labelPicture$learning = relevel(labelPicture$learning, ref = "LF")
labelPicture$frequency = relevel(labelPicture$frequency, ref = "low")
labelPicture <- lizCenter(labelPicture, list("learning" , "frequency", "task"))

```

```{r}
summarySEwithin(data = labelPicture[labelPicture$rt > 100 & labelPicture$rt <=2500 & !(labelPicture$subjID %in% badsubjs),], measurevar = "acc", betweenvars = "learning", withinvars = "frequency", idvar = "subjID", conf.interval = .95)
```


```{r}
labpic_model <- glmer(acc ~  frequency.ct*learning.ct + (frequency.ct|subjID), 
         data = labelPicture[labelPicture$rt > 100 & labelPicture$rt <=2500 & !(labelPicture$subjID %in% badsubjs),], 
         family="binomial",
         control=glmerControl(optimizer = "bobyqa"))

adjusted.labpic_model = adjust_intercept_model(labpic_model, chance = log(0.33/(1-0.33)))
round(adjusted.labpic_model,5)

```

```{r}
labpic_model.emm <- emmeans(labpic_model, ~ frequency.ct* learning.ct )
contrast(labpic_model.emm, "consec",  simple = "each", combine = F, adjust = "bonferroni")

```

Contingency judgement

```{r}
plyr::revalue(as.factor(conjudge$frequency), c("25"="low"))-> conjudge$frequency;
plyr::revalue(as.factor(conjudge$frequency), c("75"="high"))-> conjudge$frequency;

conjudge$learning = relevel(conjudge$learning, ref = "FL")
conjudge$frequency = relevel(conjudge$frequency, ref = "low")
conjudge <- lizCenter(conjudge, list("learning" , "frequency"))

```


```{r  contingency judgement model}
conjudge_model <- lmer(resp ~  learning * frequency +(frequency|subjID), 
         data = conjudge[!(conjudge$subjID %in% badsubjs) & conjudge$acc==0,])

```

```{r}
car::Anova(conjudge_model)
```

```{r}
conjudge_model.emm <- emmeans(conjudge_model , ~ learning* frequency )
contrast(conjudge_model.emm, "consec",  simple = "each", combine = F, adjust = "bonferroni")
```


## Combine both generalization tasks in one dataset

I'm going to combine both generalization tasks in one single dataset called genTask
```{r}
genTask <- rbind(labelPicture[labelPicture$rt > 100 & !(labelPicture$subjID %in% badsubjs),], 
                 pictureLabel[pictureLabel$rt > 100 & !(pictureLabel$subjID %in% badsubjs),])

genTask$frequency <- as.factor(genTask$frequency)
plyr::revalue(genTask$frequency, c("25"="low"))-> genTask$frequency;
plyr::revalue(genTask$frequency, c("75"="high"))-> genTask$frequency;


```


Relevel the variables:

```{r}
genTask$learning = relevel(genTask$learning, ref = "LF")
genTask$frequency = relevel(genTask$frequency, ref = "low")
genTask <- lizCenter(genTask, list("learning" , "frequency", "task"))
```

## The model

```{r model generalization tasks together}
genTask_model <- glmer(acc ~  frequency.ct*learning.ct + task.ct + (frequency.ct|subjID) , 
         data = genTask, 
         family="binomial",
         control=glmerControl(optimizer = "bobyqa"))

adjusted.genTask_model = adjust_intercept_model(genTask_model, chance = log(0.33/(1-0.33)))
round(adjusted.genTask_model,5)
```

Further inspection:
```{r}
genTask_model.emm <- emmeans(genTask_model , ~ frequency.ct * learning.ct )
contrast(genTask_model.emm, "consec",  simple = "each", combine = F, adjust = "bonferroni")
```

Okay, with both tasks together the take home message is the following:

- Main effect of frequency, with high frequency having higher accuracy than low frequency in both learnings.

- Main effect of learning, with FL learning having higher accuracy in the high frequency condition.

- No difference between learnings in the low frequency condition.

- No difference between tasks

```{r}
genTask %>%
  group_by(frequency, learning) %>%
  summarise(mean = mean(acc))
```

```{r}
summarySEwithin(data = genTask, measurevar = "acc", betweenvars = "learning", withinvars = "frequency", idvar = "subjID", conf.interval = .95)
```




I'm going to create a table with the estimates:
```{r}
genTask_bf = data.frame(
    condition = c(
                   "frequency by learning",
                   "learning",
                   "frequency",
                   "task"
                   ),
               
    meandiff = c(
      round(summary(genTask_model)$coefficients["frequency.ct:learning.ct", "Estimate"],3),
       round(summary(genTask_model)$coefficients["learning.ct", "Estimate"],3),
       round(summary(genTask_model)$coefficients["frequency.ct", "Estimate"],3),
       round(summary(genTask_model)$coefficients["task.ct", "Estimate"],3)
       ),
    
    se = c(
      round(summary(genTask_model)$coefficients["frequency.ct:learning.ct", "Std. Error"],3),
       round(summary(genTask_model)$coefficients["learning.ct", "Std. Error"],3),
       round(summary(genTask_model)$coefficients["frequency.ct", "Std. Error"],3),
       round(summary(genTask_model)$coefficients["task.ct", "Std. Error"],3)
       )
)

genTask_bf
```

## BF for Frequency:
```{r}
Bf(sd = genTask_bf[genTask_bf$condition=='frequency',]$se, 
   obtained = genTask_bf[genTask_bf$condition=='frequency',]$meandiff, 
   uniform = 0, 
   sdtheory = highfreq_sd, 
   meanoftheory = frequency_beta, 
   tail = 1)
```

## BF for learning:
```{r}
Bf(sd = genTask_bf[genTask_bf$condition=='learning',]$se, 
   obtained = genTask_bf[genTask_bf$condition=='learning',]$meandiff, 
   uniform = 0, 
   sdtheory = LF_sd, 
   meanoftheory = learning_beta, 
   tail = 1)
```

## BF for the interaction frequency by learning 
```{r}
Bf(sd = genTask_bf[genTask_bf$condition=='frequency by learning',]$se, 
   obtained = genTask_bf[genTask_bf$condition=='frequency by learning',]$meandiff, 
   uniform = 0, 
   sdtheory = LF_sd, #don't know how to compute sd of the interaction
   meanoftheory = freqBylearning_beta, 
   tail = 1)
```

```{r}
rm(speedacc, n, lowfreq_mean, highfreq_mean, lowfreq_sd, highfreq_sd, LF_mean, FL_mean, LF_sd, FL_sd)
```

# Summary of the results

```{r, echo=FALSE}
length(unique(learning$subjID))-> n
length(unique(learning[learning$learning=="FL",]$subjID))->n1
length(unique(learning[learning$learning=="LF",]$subjID))->n2
length(unique(pictureLabel[pictureLabel$learning=="FL" & !(pictureLabel$subjID %in% badsubjs),]$subjID))->n3
length(unique(pictureLabel[pictureLabel$learning=="LF" & !(pictureLabel$subjID %in% badsubjs),]$subjID))->n4
```

We have collected `r n` participants. Among these, `r n1` FL learning and `r n2` LF learning. 

We had four tasks: 

- Picture label task

- Label picture task

- Contingency judgement task

- Random dot task (attention check)

Participants that scored <=.5 accuracy and had >3 timeouts in the attention check (random dot task) were removed from the analysis. Participants that skipped completely one of the tasks were removed. Participants that had very few datapoints, i.e., less than 1/2 also removed. In total for picture label task we had `r n3` for FL learning, and `r n4` for LF learning.

Raw means/sd for the effects.

Label Picture:

```{r, echo=FALSE}
summaryLP<-summarySEwithin(data = labelPicture[labelPicture$rt > 100 & labelPicture$rt <=2500 & !(labelPicture$subjID %in% badsubjs),], measurevar = "acc", betweenvars = "learning", withinvars = "frequency", idvar = "subjID", conf.interval = .95)
rm(n, n1, n2, n3, n4)
summaryLP
```

Picture Label:
```{r, echo=FALSE}
summaryPL<-summarySEwithin(data = pictureLabel[pictureLabel$rt > 100 & pictureLabel$rt <=2500 & !(pictureLabel$subjID %in% badsubjs),], measurevar = "acc", betweenvars = "learning", withinvars = "frequency", idvar = "subjID", conf.interval = .95)
summaryPL

```

Data Visualization:
```{r, fig.height = 4, fig.width = 7, fig.align= "center", echo=FALSE}
ggarrange(lp, pl)
```

GLMMs models:

Picture label
```{r, echo=FALSE}
round(adjusted.piclab_model,5)
```

Label picture
```{r, echo=FALSE}
round(adjusted.labpic_model,5)
```

**What we have learned from these data:**

- Main effect of frequency, with high frequency having higher accuracy than low frequency in both learnings.

- Marginal effect of learning in the pictureLabel task, with FL learning having higher accuracy in the low frequency condition.

- No difference between learnings in the high frequency condition, although there is a trend for FL being higher than LF in the pictureLabel task

- What's important here is that the two tasks seems to behave completely differently. 


This means that the effect of frequency (high vs low) was super robust, and this is the only thing that we have replicated 100%. The difference between learnings unfortunately wasn't there, although we see a trend in this direction in one task, but not the other. Why is this the case?


**How do we explain these results:** We don't know for sure, however, throughout this experiment we have realised several important details that are not identical to the FLO paper and therefore could have affected the results:

- Learning: stimuli were pseudo-randomised with exemplars belonging to high and low frequency category of one category never displayed consequentially. 

- The whole FLO experiment was visual, not audio, therefore this might cause less ambiguity, i.e., higher accuracy, and perfect balance in the test tasks for the trial duration. Also, this would remove the confound due to the addition of the sentence, in fact, we speculated that the two learnings varies in the contiguity between stimulus and label. I.e., FL: [fribble]+"This was a .... X" Versus LF: "This is a ....X"+[fribble] introduces two different types of lags between the presentation of the label and the stimulus. We speculated that this might cause differences, we don't know how.

- Michael suggested that participants in his original experiment did only one of the two tasks, and not both. Exposition to both tasks might cause greater noise, especially in the labelPicture task where participants see 72 different fribbles. This might cause super confusion in the participants. Indeed, I found that from the folder Mike has shared with me (later on during this experiment) the number of stimuli didn't match with the number of test trials reported in the paper. 

**What we're going to do next:** We're goint to re-do the replication! This time for real: by checking for the right amount of test trials, same fribbles used by Michael and same modality. 
